{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 91128,
          "databundleVersionId": 10717135,
          "sourceType": "competition"
        },
        {
          "sourceId": 10616568,
          "sourceType": "datasetVersion",
          "datasetId": 6573023
        }
      ],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "sjRGLEXIYCO-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "AnXWbY5XYCO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasketch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T19:46:42.637402Z",
          "iopub.execute_input": "2025-02-08T19:46:42.637808Z",
          "iopub.status.idle": "2025-02-08T19:46:48.908650Z",
          "shell.execute_reply.started": "2025-02-08T19:46:42.637773Z",
          "shell.execute_reply": "2025-02-08T19:46:48.907452Z"
        },
        "id": "6iF9QJUBYCPA",
        "outputId": "6184fd88-29fe-4a3d-d5b7-276ad695faf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting datasketch\n  Downloading datasketch-1.6.5-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from datasketch) (1.26.4)\nRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasketch) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->datasketch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->datasketch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->datasketch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->datasketch) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->datasketch) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->datasketch) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.11->datasketch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.11->datasketch) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.11->datasketch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.11->datasketch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.11->datasketch) (2024.2.0)\nDownloading datasketch-1.6.5-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: datasketch\nSuccessfully installed datasketch-1.6.5\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import spacy\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from scipy.spatial.distance import jaccard\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "print(\"Imports have been run...\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T19:46:49.779900Z",
          "iopub.execute_input": "2025-02-08T19:46:49.780289Z",
          "iopub.status.idle": "2025-02-08T19:46:49.803722Z",
          "shell.execute_reply.started": "2025-02-08T19:46:49.780252Z",
          "shell.execute_reply": "2025-02-08T19:46:49.802574Z"
        },
        "id": "vG3PTGqXYCPA",
        "outputId": "fb1384b8-3a5e-493a-d34f-f0f2cdff9755"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Imports have been run...\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "### Read datasets"
      ],
      "metadata": {
        "id": "mJ27zgNdYCPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/kaggle/input/bigdata2024classification/train.csv\")\n",
        "test_df = pd.read_csv(\"/kaggle/input/bigdata2024classification/test_without_labels.csv\")\n",
        "\n",
        "print(\"Files loaded...\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T19:46:52.573295Z",
          "iopub.execute_input": "2025-02-08T19:46:52.573655Z",
          "iopub.status.idle": "2025-02-08T19:47:03.590787Z",
          "shell.execute_reply.started": "2025-02-08T19:46:52.573626Z",
          "shell.execute_reply": "2025-02-08T19:47:03.589777Z"
        },
        "id": "pBTupS5YYCPA",
        "outputId": "f5d8ebf1-9eaa-4400-fa6a-d1af113e6ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Files loaded...\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualise label distribution in the dataset"
      ],
      "metadata": {
        "id": "LigT0jGpYCPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = train_df['Label'].value_counts()\n",
        "\n",
        "label_counts.plot(kind='bar')\n",
        "plt.title('Label distribution in train set')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "sis9HYAIYCPB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset cleanup and pre-processing"
      ],
      "metadata": {
        "id": "GvVOaWrAYCPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words + Scaling"
      ],
      "metadata": {
        "id": "rePV1qegYCPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with empty content or label\n",
        "train_df = train_df.dropna(subset=['Content', 'Label'])\n",
        "\n",
        "# Transform all content to lower case\n",
        "train_df['Content'] = train_df['Content'].str.lower()\n",
        "test_df['Content'] = test_df['Content'].str.lower()\n",
        "\n",
        "print(\"Cleanup done...\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T19:47:10.095340Z",
          "iopub.execute_input": "2025-02-08T19:47:10.095710Z",
          "iopub.status.idle": "2025-02-08T19:47:10.218795Z",
          "shell.execute_reply.started": "2025-02-08T19:47:10.095682Z",
          "shell.execute_reply": "2025-02-08T19:47:10.217384Z"
        },
        "id": "yyB3ja88YCPB",
        "outputId": "0d0216e0-85ff-4ad8-8020-fa39fccdb7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Cleanup done...\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes for max features and lemmatization\n",
        "\n",
        "#### Max Features\n",
        "* 1000: Lower 90s percentage, good performance\n",
        "* 5000: Best results\n",
        "* 10000: accuracy declines again\n",
        "\n",
        "#### Lemmatization\n",
        "In all the feature combinations it does not seem to improve accuracy. Just doubles the total computation time including the fitting of the SVM and Forest models."
      ],
      "metadata": {
        "id": "cR1A0nPEYCPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
        "vectorizer_big = CountVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "count_vect = vectorizer.fit_transform(train_df['Content'])\n",
        "count_vect_test = vectorizer.transform(test_df['Content'])\n",
        "\n",
        "count_vect_big = vectorizer_big.fit_transform(train_df['Content'])\n",
        "count_vect_test_big = vectorizer_big.transform(test_df['Content'])\n",
        "\n",
        "\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "\n",
        "count_vect_scaled = scaler.fit_transform(count_vect)\n",
        "count_vect_test_scaled = scaler.transform(count_vect_test)\n",
        "\n",
        "count_vect_big_scaled = scaler.fit_transform(count_vect_big)\n",
        "count_vect_test_big_scaled = scaler.transform(count_vect_test_big)\n",
        "\n",
        "print(\"Bag of words and scaling done...\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T19:47:13.338926Z",
          "iopub.execute_input": "2025-02-08T19:47:13.339335Z",
          "iopub.status.idle": "2025-02-08T19:47:17.307888Z",
          "shell.execute_reply.started": "2025-02-08T19:47:13.339297Z",
          "shell.execute_reply": "2025-02-08T19:47:17.306679Z"
        },
        "id": "sFZfBKvPYCPB",
        "outputId": "22a5c3d9-4a8f-46d9-ef82-a3861b839c74"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Bag of words and scaling done...\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "oVw8kErsYCPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machine (SVM)\n",
        "# LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel\n",
        "# It implements “one-vs-the-rest” multi-class strategy\n",
        "svm_model = LinearSVC(max_iter=10000)\n",
        "\n",
        "# Perform 5-fold cross-validation and get predictions for each fold\n",
        "prediction = cross_val_predict(svm_model, count_vect_scaled, train_df['Label'], cv=5)\n",
        "\n",
        "# Print classification report for each category\n",
        "print(\"========== SVM Classification Report ==========\\n\")\n",
        "print(classification_report(train_df['Label'], prediction))\n",
        "\n",
        "# Perform 5-fold cross-validation and get predictions for each fold\n",
        "prediction = cross_val_predict(svm_model, count_vect_big_scaled, train_df['Label'], cv=5)\n",
        "\n",
        "# Print classification report for each category\n",
        "print(\"========== SVM Classification Report - More Features ==========\\n\")\n",
        "print(classification_report(train_df['Label'], prediction))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T19:47:21.826409Z",
          "iopub.execute_input": "2025-02-08T19:47:21.826802Z",
          "iopub.status.idle": "2025-02-08T19:48:05.399689Z",
          "shell.execute_reply.started": "2025-02-08T19:47:21.826767Z",
          "shell.execute_reply": "2025-02-08T19:48:05.398426Z"
        },
        "id": "zBAb3yL6YCPB",
        "outputId": "4edf63d0-629b-4d06-d7f1-6020b6c1ae96"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "========== SVM Classification Report ==========\n\n               precision    recall  f1-score   support\n\n     Business       0.80      0.76      0.78     24834\nEntertainment       0.84      0.94      0.89     44834\n       Health       0.83      0.68      0.75     12020\n   Technology       0.85      0.80      0.82     30107\n\n     accuracy                           0.83    111795\n    macro avg       0.83      0.79      0.81    111795\n weighted avg       0.83      0.83      0.83    111795\n\n========== SVM Classification Report - More Features ==========\n\n               precision    recall  f1-score   support\n\n     Business       0.88      0.88      0.88     24834\nEntertainment       0.96      0.97      0.97     44834\n       Health       0.92      0.89      0.90     12020\n   Technology       0.90      0.91      0.90     30107\n\n     accuracy                           0.92    111795\n    macro avg       0.92      0.91      0.91    111795\n weighted avg       0.92      0.92      0.92    111795\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "GtcblBhoYCPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "# A random forest is a meta estimator that fits a number of decision tree classifiers (estimators) on various sub-samples of the dataset\n",
        "# and uses averaging to improve the predictive accuracy and control over-fitting.\n",
        "rf_model = RandomForestClassifier(n_estimators=100, n_jobs=-1) # n_jobs=-1 for using all available processors\n",
        "\n",
        "# Perform 5-fold cross-validation and get predictions for each fold\n",
        "prediction = cross_val_predict(rf_model, count_vect_scaled, train_df['Label'], cv=5)\n",
        "\n",
        "# Print classification report for each category\n",
        "print(\"========== Random Forest Classification Report ==========\\n\")\n",
        "print(classification_report(train_df['Label'], prediction))\n",
        "\n",
        "# Perform 5-fold cross-validation and get predictions for each fold\n",
        "prediction = cross_val_predict(rf_model, count_vect_big_scaled, train_df['Label'], cv=5)\n",
        "\n",
        "# Print classification report for each category\n",
        "print(\"========== Random Forest Classification Report - More Features ==========\\n\")\n",
        "print(classification_report(train_df['Label'], prediction))"
      ],
      "metadata": {
        "trusted": true,
        "id": "k103l52zYCPC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Test Set with the best performing model"
      ],
      "metadata": {
        "id": "oLbXhZovYCPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The best performing model was the SVM with 5000 features\n",
        "# It had 95% accuracy\n",
        "\n",
        "svm_model = LinearSVC(max_iter=10000)\n",
        "\n",
        "svm_model.fit(count_vect_big_scaled, train_df['Label'])\n",
        "print(\"Training complete...\")\n",
        "\n",
        "prediction = svm_model.predict(count_vect_test_big_scaled)\n",
        "print(\"Prediction of test set complete...\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "M9uIFKgdYCPC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open('testSet_categories.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Id\",\"Predicted\"])\n",
        "\n",
        "    for i, label in enumerate(prediction):\n",
        "        writer.writerow([test_df['Id'][i], label])\n",
        "\n",
        "print(\"CSV created...\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "wGehvS5HYCPC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nearest Neigbours w/ Jaccard - Brute"
      ],
      "metadata": {
        "id": "pgfZQVMXYCPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the train set to measure F1 Score for Question 1 --- Comment out when not wanting to calculate F1 Score of Brute Force\n",
        "# train_x, test_x, train_y, test_y = train_test_split(train_df['Content'], train_df['Label'], test_size=0.1, random_state=0)\n",
        "\n",
        "# The assignment's original datasets -- Comment out when wanting to calculate F1 Score of Brute Force\n",
        "train_x, test_x = train_df['Content'], test_df['Content']\n",
        "\n",
        "\n",
        "# Revectorize the training and testing sets for clarity\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "\n",
        "count_vect_train = vectorizer.fit_transform(train_x)\n",
        "count_vect_test = vectorizer.transform(test_x)\n",
        "\n",
        "# Create an analyzer to use for the LSH\n",
        "analyzer = vectorizer.build_analyzer()\n",
        "\n",
        "print(\"KNN pre-processing done...\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T10:28:35.079841Z",
          "iopub.execute_input": "2025-02-05T10:28:35.080213Z",
          "iopub.status.idle": "2025-02-05T10:28:35.104003Z",
          "shell.execute_reply.started": "2025-02-05T10:28:35.080187Z",
          "shell.execute_reply": "2025-02-05T10:28:35.102924Z"
        },
        "id": "qkuNuSX-YCPC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the vects dense\n",
        "count_vect_train = count_vect_train.toarray()\n",
        "count_vect_test = count_vect_test.toarray()\n",
        "\n",
        "print(\"Vects became dense...\")\n",
        "\n",
        "# Make the vects boolean for jaccard\n",
        "count_vect_train = count_vect_train.astype(bool)\n",
        "count_vect_test = count_vect_test.astype(bool)\n",
        "\n",
        "print(\"Vects became bool for jaccard...\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "nI0RPtcgYCPC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Start counting time here for building\n",
        "start_time = time.time()\n",
        "\n",
        "knn_model = NearestNeighbors(n_neighbors=7, algorithm='brute', metric='jaccard', n_jobs=-1)\n",
        "\n",
        "knn_model.fit(count_vect_train)\n",
        "\n",
        "# Build time after fitting the knn_model\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(\"KNN fitting completed...\")\n",
        "print(f\"Build time: {build_time} seconds\")\n",
        "\n",
        "# Start counting time here for querying\n",
        "start_time = time.time()\n",
        "\n",
        "distances, indices = knn_model.kneighbors(count_vect_test)\n",
        "\n",
        "print (\"Finished querying...\")\n",
        "\n",
        "# Querying time\n",
        "query_time = time.time() - start_time\n",
        "print(f\"Query time: {query_time} seconds\")\n",
        "\n",
        "# Total time\n",
        "total_time = build_time + query_time\n",
        "print(f\"Total time: {total_time} seconds\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "BkqewvypYCPC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate F1 Metric"
      ],
      "metadata": {
        "id": "w7kzNfJhYCPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment out when running with the assignment's original datasets\n",
        "\n",
        "# # Convert neighbor indices to labels using majority voting\n",
        "# predicted_labels = []\n",
        "# for i in range(len(indices)):\n",
        "#     neighbor_labels = train_y.iloc[indices[i]]  # Get the labels of nearest neighbors\n",
        "#     predicted_label = max(set(neighbor_labels), key=list(neighbor_labels).count)  # Majority voting\n",
        "#     predicted_labels.append(predicted_label)\n",
        "\n",
        "# # Compute F1-score\n",
        "# f1 = f1_score(test_y, predicted_labels, average='weighted')\n",
        "# print(f\"KNN F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "HsTwUkj2YCPC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nearest Neigbours w/ Jaccard - MinHash LSH"
      ],
      "metadata": {
        "id": "Fm9rs77rYCPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create MinHash functions\n",
        "def get_minhash(content, num_perm):\n",
        "    m = MinHash(num_perm=num_perm)\n",
        "    words = analyzer(content)\n",
        "\n",
        "    for word in words:\n",
        "        m.update(word.encode('utf8'))\n",
        "    return m\n",
        "\n",
        "# Query the LSH index for similar content\n",
        "def query_lsh_index(content, lsh, minhashes, num_perm, num_candidates=7):\n",
        "    # Create MinHash for the content\n",
        "    content_minhash = get_minhash(content, num_perm)\n",
        "\n",
        "    # Query the LSH index for the most similar contents\n",
        "    similar_contents = lsh.query(content_minhash)\n",
        "\n",
        "    # Only return the top 'num_candidates' documents from the LSH results - For the question's specification 7\n",
        "    similar_contents = similar_contents[:num_candidates]\n",
        "\n",
        "    # Retrieve the corresponding documents from the minhashes dictionary\n",
        "    similar_contents = [(i, minhashes[i]) for i in similar_contents]\n",
        "\n",
        "    return similar_contents"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T10:28:53.426991Z",
          "iopub.execute_input": "2025-02-05T10:28:53.427366Z",
          "iopub.status.idle": "2025-02-05T10:28:59.659163Z",
          "shell.execute_reply.started": "2025-02-05T10:28:53.427306Z",
          "shell.execute_reply": "2025-02-05T10:28:59.657709Z"
        },
        "id": "LUjh5JQBYCPC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different params cells"
      ],
      "metadata": {
        "id": "bIPJvKN9YCPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#===== BUILD =====#\n",
        "\n",
        "threshold = 0.8\n",
        "num_perm = 16\n",
        "\n",
        "# Start counting time here for building\n",
        "start_time = time.time()\n",
        "\n",
        "# Create LSH Index\n",
        "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "\n",
        "# Create MinHash\n",
        "minhashes = {}\n",
        "for i, content in enumerate(train_x):\n",
        "    minhashes[i] = get_minhash(content, num_perm)\n",
        "    lsh.insert(i, minhashes[i])\n",
        "\n",
        "# Build time after fitting the knn_model\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(f\"Finished building LSH Index and Minhashes (threshold={threshold}, num_perm={num_perm})...\")\n",
        "print(f\"Build time: {build_time} seconds\")\n",
        "\n",
        "#===== QUERY =====#\n",
        "\n",
        "# Start counting time here for querying\n",
        "start_time = time.time()\n",
        "\n",
        "lsh_results = []\n",
        "for (i, content) in enumerate(test_x):\n",
        "    similar_docs = query_lsh_index(content, lsh, minhashes, num_perm, num_candidates=7)\n",
        "    lsh_results.append(similar_docs)\n",
        "\n",
        "print (\"Finished querying...\")\n",
        "\n",
        "# Querying time\n",
        "query_time = time.time() - start_time\n",
        "print(f\"Query time: {query_time} seconds\")\n",
        "\n",
        "# Total time\n",
        "total_time = build_time + query_time\n",
        "print(f\"Total time: {total_time} seconds\")\n",
        "\n",
        "#===== ASSESS =====#\n",
        "\n",
        "correct_matches = 0\n",
        "total_retrieved = 0\n",
        "\n",
        "for i, brute_neighbors in enumerate(indices):\n",
        "    lsh_neighbors = [doc_id for doc_id, _ in lsh_results[i]]  # Extract only document IDs\n",
        "    correct_matches += len(set(map(int, lsh_neighbors)) & set(map(int, brute_neighbors)))\n",
        "    total_retrieved += len(lsh_neighbors)\n",
        "\n",
        "precision = correct_matches / total_retrieved if total_retrieved > 0 else 0\n",
        "\n",
        "print(f\"Precision for threshold={threshold} and num_perm={num_perm}: {precision}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9lj_sA4UYCPC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#===== BUILD =====#\n",
        "\n",
        "threshold = 0.9\n",
        "num_perm = 32\n",
        "\n",
        "# Start counting time here for building\n",
        "start_time = time.time()\n",
        "\n",
        "# Create LSH Index\n",
        "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "\n",
        "# Create MinHash\n",
        "minhashes = {}\n",
        "for i, content in enumerate(train_x):\n",
        "    minhashes[i] = get_minhash(content, num_perm)\n",
        "    lsh.insert(i, minhashes[i])\n",
        "\n",
        "# Build time after fitting the knn_model\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(f\"Finished building LSH Index and Minhashes (threshold={threshold}, num_perm={num_perm})...\")\n",
        "print(f\"Build time: {build_time} seconds\")\n",
        "\n",
        "#===== QUERY =====#\n",
        "\n",
        "# Start counting time here for querying\n",
        "start_time = time.time()\n",
        "\n",
        "lsh_results = []\n",
        "for (i, content) in enumerate(test_x):\n",
        "    similar_docs = query_lsh_index(content, lsh, minhashes, num_perm, num_candidates=7)\n",
        "    lsh_results.append(similar_docs)\n",
        "\n",
        "print (\"Finished querying...\")\n",
        "\n",
        "# Querying time\n",
        "query_time = time.time() - start_time\n",
        "print(f\"Query time: {query_time} seconds\")\n",
        "\n",
        "# Total time\n",
        "total_time = build_time + query_time\n",
        "print(f\"Total time: {total_time} seconds\")\n",
        "\n",
        "#===== ASSESS =====#\n",
        "\n",
        "correct_matches = 0\n",
        "total_retrieved = 0\n",
        "\n",
        "for i, brute_neighbors in enumerate(indices):\n",
        "    lsh_neighbors = [doc_id for doc_id, _ in lsh_results[i]]  # Extract only document IDs\n",
        "    correct_matches += len(set(map(int, lsh_neighbors)) & set(map(int, brute_neighbors)))\n",
        "    total_retrieved += len(lsh_neighbors)\n",
        "\n",
        "precision = correct_matches / total_retrieved if total_retrieved > 0 else 0\n",
        "\n",
        "print(f\"Precision for threshold={threshold} and num_perm={num_perm}: {precision}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9RQ9ZlVGYCPD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#===== BUILD =====#\n",
        "\n",
        "threshold = 0.9\n",
        "num_perm = 64\n",
        "\n",
        "# Start counting time here for building\n",
        "start_time = time.time()\n",
        "\n",
        "# Create LSH Index\n",
        "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "\n",
        "# Create MinHash\n",
        "minhashes = {}\n",
        "for i, content in enumerate(train_x):\n",
        "    minhashes[i] = get_minhash(content, num_perm)\n",
        "    lsh.insert(i, minhashes[i])\n",
        "\n",
        "# Build time after fitting the knn_model\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(f\"Finished building LSH Index and Minhashes (threshold={threshold}, num_perm={num_perm})...\")\n",
        "print(f\"Build time: {build_time} seconds\")\n",
        "\n",
        "#===== QUERY =====#\n",
        "\n",
        "# Start counting time here for querying\n",
        "start_time = time.time()\n",
        "\n",
        "lsh_results = []\n",
        "for (i, content) in enumerate(test_x):\n",
        "    similar_docs = query_lsh_index(content, lsh, minhashes, num_perm, num_candidates=7)\n",
        "    lsh_results.append(similar_docs)\n",
        "\n",
        "print (\"Finished querying...\")\n",
        "\n",
        "# Querying time\n",
        "query_time = time.time() - start_time\n",
        "print(f\"Query time: {query_time} seconds\")\n",
        "\n",
        "# Total time\n",
        "total_time = build_time + query_time\n",
        "print(f\"Total time: {total_time} seconds\")\n",
        "\n",
        "#===== ASSESS =====#\n",
        "\n",
        "correct_matches = 0\n",
        "total_retrieved = 0\n",
        "\n",
        "for i, brute_neighbors in enumerate(indices):\n",
        "    lsh_neighbors = [doc_id for doc_id, _ in lsh_results[i]]  # Extract only document IDs\n",
        "    correct_matches += len(set(map(int, lsh_neighbors)) & set(map(int, brute_neighbors)))\n",
        "    total_retrieved += len(lsh_neighbors)\n",
        "\n",
        "precision = correct_matches / total_retrieved if total_retrieved > 0 else 0\n",
        "\n",
        "print(f\"Precision for threshold={threshold} and num_perm={num_perm}: {precision}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "tcYEbf77YCPD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#===== BUILD =====#\n",
        "\n",
        "threshold = 0.7\n",
        "num_perm = 32\n",
        "\n",
        "# Start counting time here for building\n",
        "start_time = time.time()\n",
        "\n",
        "# Create LSH Index\n",
        "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "\n",
        "# Create MinHash\n",
        "minhashes = {}\n",
        "for i, content in enumerate(train_x):\n",
        "    minhashes[i] = get_minhash(content, num_perm)\n",
        "    lsh.insert(i, minhashes[i])\n",
        "\n",
        "# Build time after fitting the knn_model\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(f\"Finished building LSH Index and Minhashes (threshold={threshold}, num_perm={num_perm})...\")\n",
        "print(f\"Build time: {build_time} seconds\")\n",
        "\n",
        "#===== QUERY =====#\n",
        "\n",
        "# Start counting time here for querying\n",
        "start_time = time.time()\n",
        "\n",
        "lsh_results = []\n",
        "for (i, content) in enumerate(test_x):\n",
        "    similar_docs = query_lsh_index(content, lsh, minhashes, num_perm, num_candidates=7)\n",
        "    lsh_results.append(similar_docs)\n",
        "\n",
        "print (\"Finished querying...\")\n",
        "\n",
        "# Querying time\n",
        "query_time = time.time() - start_time\n",
        "print(f\"Query time: {query_time} seconds\")\n",
        "\n",
        "# Total time\n",
        "total_time = build_time + query_time\n",
        "print(f\"Total time: {total_time} seconds\")\n",
        "\n",
        "#===== ASSESS =====#\n",
        "\n",
        "correct_matches = 0\n",
        "total_retrieved = 0\n",
        "\n",
        "for i, brute_neighbors in enumerate(indices):\n",
        "    lsh_neighbors = [doc_id for doc_id, _ in lsh_results[i]]  # Extract only document IDs\n",
        "    correct_matches += len(set(map(int, lsh_neighbors)) & set(map(int, brute_neighbors)))\n",
        "    total_retrieved += len(lsh_neighbors)\n",
        "\n",
        "precision = correct_matches / total_retrieved if total_retrieved > 0 else 0\n",
        "\n",
        "print(f\"Precision for threshold={threshold} and num_perm={num_perm}: {precision}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LJoWnVMbYCPD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#===== BUILD =====#\n",
        "\n",
        "threshold = 0.7\n",
        "num_perm = 64\n",
        "\n",
        "# Start counting time here for building\n",
        "start_time = time.time()\n",
        "\n",
        "# Create LSH Index\n",
        "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "\n",
        "# Create MinHash\n",
        "minhashes = {}\n",
        "for i, content in enumerate(train_x):\n",
        "    minhashes[i] = get_minhash(content, num_perm)\n",
        "    lsh.insert(i, minhashes[i])\n",
        "\n",
        "# Build time after fitting the knn_model\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(f\"Finished building LSH Index and Minhashes (threshold={threshold}, num_perm={num_perm})...\")\n",
        "print(f\"Build time: {build_time} seconds\")\n",
        "\n",
        "#===== QUERY =====#\n",
        "\n",
        "# Start counting time here for querying\n",
        "start_time = time.time()\n",
        "\n",
        "lsh_results = []\n",
        "for (i, content) in enumerate(test_x):\n",
        "    similar_docs = query_lsh_index(content, lsh, minhashes, num_perm, num_candidates=7)\n",
        "    lsh_results.append(similar_docs)\n",
        "\n",
        "print (\"Finished querying...\")\n",
        "\n",
        "# Querying time\n",
        "query_time = time.time() - start_time\n",
        "print(f\"Query time: {query_time} seconds\")\n",
        "\n",
        "# Total time\n",
        "total_time = build_time + query_time\n",
        "print(f\"Total time: {total_time} seconds\")\n",
        "\n",
        "#===== ASSESS =====#\n",
        "\n",
        "correct_matches = 0\n",
        "total_retrieved = 0\n",
        "\n",
        "for i, brute_neighbors in enumerate(indices):\n",
        "    lsh_neighbors = [doc_id for doc_id, _ in lsh_results[i]]  # Extract only document IDs\n",
        "    correct_matches += len(set(map(int, lsh_neighbors)) & set(map(int, brute_neighbors)))\n",
        "    total_retrieved += len(lsh_neighbors)\n",
        "\n",
        "precision = correct_matches / total_retrieved if total_retrieved > 0 else 0\n",
        "\n",
        "print(f\"Precision for threshold={threshold} and num_perm={num_perm}: {precision}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "n7MnNWarYCPD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#===== BUILD =====#\n",
        "\n",
        "threshold = 0.5\n",
        "num_perm = 16\n",
        "\n",
        "# Start counting time here for building\n",
        "start_time = time.time()\n",
        "\n",
        "# Create LSH Index\n",
        "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "\n",
        "# Create MinHash\n",
        "minhashes = {}\n",
        "for i, content in enumerate(train_x):\n",
        "    minhashes[i] = get_minhash(content, num_perm)\n",
        "    lsh.insert(i, minhashes[i])\n",
        "\n",
        "# Build time after fitting the knn_model\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(f\"Finished building LSH Index and Minhashes (threshold={threshold}, num_perm={num_perm})...\")\n",
        "print(f\"Build time: {build_time} seconds\")\n",
        "\n",
        "#===== QUERY =====#\n",
        "\n",
        "# Start counting time here for querying\n",
        "start_time = time.time()\n",
        "\n",
        "lsh_results = []\n",
        "for (i, content) in enumerate(test_x):\n",
        "    similar_docs = query_lsh_index(content, lsh, minhashes, num_perm, num_candidates=7)\n",
        "    lsh_results.append(similar_docs)\n",
        "\n",
        "print (\"Finished querying...\")\n",
        "\n",
        "# Querying time\n",
        "query_time = time.time() - start_time\n",
        "print(f\"Query time: {query_time} seconds\")\n",
        "\n",
        "# Total time\n",
        "total_time = build_time + query_time\n",
        "print(f\"Total time: {total_time} seconds\")\n",
        "\n",
        "#===== ASSESS =====#\n",
        "\n",
        "correct_matches = 0\n",
        "total_retrieved = 0\n",
        "\n",
        "for i, brute_neighbors in enumerate(indices):\n",
        "    lsh_neighbors = [doc_id for doc_id, _ in lsh_results[i]]  # Extract only document IDs\n",
        "    correct_matches += len(set(map(int, lsh_neighbors)) & set(map(int, brute_neighbors)))\n",
        "    total_retrieved += len(lsh_neighbors)\n",
        "\n",
        "precision = correct_matches / total_retrieved if total_retrieved > 0 else 0\n",
        "\n",
        "print(f\"Precision for threshold={threshold} and num_perm={num_perm}: {precision}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LFWnSmcxYCPD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#===== BUILD =====#\n",
        "\n",
        "threshold = 0.5\n",
        "num_perm = 64\n",
        "\n",
        "# Start counting time here for building\n",
        "start_time = time.time()\n",
        "\n",
        "# Create LSH Index\n",
        "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "\n",
        "# Create MinHash\n",
        "minhashes = {}\n",
        "for i, content in enumerate(train_x):\n",
        "    minhashes[i] = get_minhash(content, num_perm)\n",
        "    lsh.insert(i, minhashes[i])\n",
        "\n",
        "# Build time after fitting the knn_model\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(f\"Finished building LSH Index and Minhashes (threshold={threshold}, num_perm={num_perm})...\")\n",
        "print(f\"Build time: {build_time} seconds\")\n",
        "\n",
        "#===== QUERY =====#\n",
        "\n",
        "# Start counting time here for querying\n",
        "start_time = time.time()\n",
        "\n",
        "lsh_results = []\n",
        "for (i, content) in enumerate(test_x):\n",
        "    similar_docs = query_lsh_index(content, lsh, minhashes, num_perm, num_candidates=7)\n",
        "    lsh_results.append(similar_docs)\n",
        "\n",
        "print (\"Finished querying...\")\n",
        "\n",
        "# Querying time\n",
        "query_time = time.time() - start_time\n",
        "print(f\"Query time: {query_time} seconds\")\n",
        "\n",
        "# Total time\n",
        "total_time = build_time + query_time\n",
        "print(f\"Total time: {total_time} seconds\")\n",
        "\n",
        "#===== ASSESS =====#\n",
        "\n",
        "correct_matches = 0\n",
        "total_retrieved = 0\n",
        "\n",
        "for i, brute_neighbors in enumerate(indices):\n",
        "    lsh_neighbors = [doc_id for doc_id, _ in lsh_results[i]]  # Extract only document IDs\n",
        "    correct_matches += len(set(map(int, lsh_neighbors)) & set(map(int, brute_neighbors)))\n",
        "    total_retrieved += len(lsh_neighbors)\n",
        "\n",
        "precision = correct_matches / total_retrieved if total_retrieved > 0 else 0\n",
        "\n",
        "print(f\"Precision for threshold={threshold} and num_perm={num_perm}: {precision}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "xPq1BXPIYCPD"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}